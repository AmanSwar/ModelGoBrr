import triton
import triton.language as tl

import torch
from torch.autograd.function import Function
from .config_tool import tuner


@triton.jit
def _qknorm_fwd_kernel(
    Q,
    K,
    outQ,
    outK,
    WeightQ,
    WeightK,
    M , N,
    eps,
    BLOCK_SIZE : tl.constexpr
):
    
    row_index = tl.program_id(0)

    row_start = row_index * N

    col_offset = tl.arange(0 , BLOCK_SIZE)

    q_input_ptrs = Q + row_start + col_offset
    k_input_ptrs = K + row_start + col_offset

    mask = col_offset < N

    #load the Q and K row
    q_row = tl.load(q_input_ptrs , mask=mask , other=0.0)
    k_row = tl.load(k_input_ptrs , mask=mask , other=0.0)

    #load the Weights of Q and K
    weight_k_row = tl.load(WeightK + col_offset , mask=mask ,other=1.0)
    weight_q_row = tl.load(WeightQ + col_offset , mask=mask ,other=1.0)


    _rms_q = tl.rsqrt(((tl.sum(q_row * q_row))/N) + eps)
    _rms_k = tl.rsqrt(((tl.sum(k_row * k_row))/N) + eps)

    q_out_row = (q_row * _rms_q) * weight_q_row
    k_out_row = (k_row * _rms_k) * weight_k_row


    q_output_ptrs = outQ + row_start + col_offset
    k_output_ptrs = outK  +row_start + col_offset
    
    tl.store(q_output_ptrs , q_out_row , mask=mask)
    tl.store(k_output_ptrs , k_out_row , mask=mask)


class QKNormTritonFunction(Function):

    @staticmethod
    def forward(
        ctx,
        Q : torch.Tensor,
        K : torch.Tensor,
        WeightQ : torch.Tensor,
        WeightK : torch.Tensor,
        eps = 1e-6
    ):
        *batch_dim_q , N_q = Q.shape
        *batch_dim_k , N_k = K.shape
        # assert((batch_dim_k == batch_dim_q) and (N_q == N_k)) , "Dimension are not same"

        M = Q.numel() // N_q

        Q_2d_view = Q.reshape(M , N_q)
        K_2d_view = K.reshape(M , N_q)

        outQ = torch.empty_like(Q_2d_view) 
        outK = torch.empty_like(K_2d_view) 

        BLOCK_SIZE , num_warps = tuner(N_q)

        grid =(M , )

        _qknorm_fwd_kernel[grid](
            Q=Q_2d_view,
            K=K_2d_view,
            outQ=outQ,
            outK=outK,
            WeightQ=WeightQ,
            WeightK=WeightK,
            M=M,
            N=N_q,
            eps=eps,
            BLOCK_SIZE=BLOCK_SIZE,
            num_warps=num_warps
        )

        return outQ.reshape(Q.shape) , outK.reshape(K.shape)


def qknorm_triton(q , k , weight_q , weight_k , eps=1e-6):
    return QKNormTritonFunction.apply(q , k , weight_q , weight_k , eps)


if __name__ == "__main__":
    # BENCHMARKS TOTALLY GENERATED BY GEMINI

    import torch.nn as nn

    bs, num_heads, seq_len, head_dim = 2, 8, 128, 64

    # Create test tensors
    q_tensor = torch.rand(
        bs, num_heads, seq_len, head_dim, device="cuda", dtype=torch.float16
    )
    k_tensor = torch.rand(
        bs, num_heads, seq_len, head_dim, device="cuda", dtype=torch.float16
    )

    # Create PyTorch RMSNorm layers to get reference outputs and weights
    q_norm_torch_layer = torch.nn.RMSNorm(
        head_dim, eps=1e-6, device="cuda", dtype=torch.float16
    )
    k_norm_torch_layer = torch.nn.RMSNorm(
        head_dim, eps=1e-6, device="cuda", dtype=torch.float16
    )

    # Get weights from the torch layers
    weight_q = q_norm_torch_layer.weight
    weight_k = k_norm_torch_layer.weight

    # Calculate reference output using PyTorch
    q_ref, k_ref = q_norm_torch_layer(q_tensor), k_norm_torch_layer(k_tensor)

    # Calculate output using the Triton kernel
    q_triton, k_triton = qknorm_triton(q_tensor, k_tensor, weight_q, weight_k) # type: ignore

    # Compare results
    # We use a larger tolerance (atol) for float16 comparisons
    assert torch.allclose(
        q_ref, q_triton, atol=1e-2, rtol=0
    ), "Triton Q output does not match PyTorch reference"
    assert torch.allclose(
        k_ref, k_triton, atol=1e-2, rtol=0
    ), "Triton K output does not match PyTorch reference"
    print("âœ… Unit Test Passed!")

    @triton.testing.perf_report([
        triton.testing.Benchmark(
            x_names=['N'],  # The parameter we are varying
            x_vals=[128, 256, 512, 1024, 2048, 4096],  # Different values for N
            line_arg='provider',  # We'll have lines for 'triton' and 'torch'
            line_vals=['triton', 'torch'],
            line_names=['Triton', 'PyTorch'],
            styles=[('blue', '-'), ('green', '-')],
            ylabel='GFLOPS',  # Label for the y-axis
            plot_name='qknorm-performance-gflops',  # Name of the plot
            args={'M': 4096},  # Fixed batch dimension for the benchmark
        )
    ])
    def benchmark(M, N, provider):
        # Create random input tensors
        q = torch.randn(M, N, device='cuda', dtype=torch.float16)
        k = torch.randn(M, N, device='cuda', dtype=torch.float16)
        weight_q = torch.randn(N, device='cuda', dtype=torch.float16)
        weight_k = torch.randn(N, device='cuda', dtype=torch.float16)
        
        # Define quantiles for measuring performance
        quantiles = [0.5, 0.2, 0.8]
        
        if provider == 'triton':
            ms, min_ms, max_ms = triton.testing.do_bench(
                lambda: qknorm_triton(q, k, weight_q, weight_k),
                quantiles=quantiles
            )
        elif provider == 'torch':
            # PyTorch equivalent using nn.functional for fair comparison
            def torch_qknorm(q, k, w_q, w_k, eps=1e-6):
                # RMSNorm is equivalent to LayerNorm without mean-centering and bias
                q_norm = torch.nn.functional.layer_norm(q, (N,), w_q, None, eps) 
                k_norm = torch.nn.functional.layer_norm(k, (N,), w_k, None, eps)
                return q_norm, k_norm
                
            ms, min_ms, max_ms = triton.testing.do_bench(
                lambda: torch_qknorm(q, k, weight_q, weight_k),
                quantiles=quantiles
            )
            
        total_flops = 8 * M * N

        def to_gflops(time_ms):
            if time_ms == 0:
                return float('inf')
            # GFLOPS = (total_operations / seconds) / 10^9
            return (total_flops / (time_ms * 1e-3)) / 1e9

        return to_gflops(ms), to_gflops(max_ms), to_gflops(min_ms)

    print("\nRunning Benchmark...")
    benchmark.run(print_data=True, show_plots=True)
